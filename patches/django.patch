--- a/django/conf/global_settings.py
+++ b/django/conf/global_settings.py
@@ -299,6 +299,10 @@ DATA_UPLOAD_MAX_MEMORY_SIZE = 2621440  # i.e. 2.5 MB
 # SuspiciousOperation (TooManyFieldsSent) is raised.
 DATA_UPLOAD_MAX_NUMBER_FIELDS = 1000
 
+# Maximum number of files encoded in a multipart upload that will be read
+# before a SuspiciousOperation (TooManyFilesSent) is raised.
+DATA_UPLOAD_MAX_NUMBER_FILES = 100
+
 # Directory in which upload streamed files will be temporarily saved. A value of
 # `None` will make Django use the operating system's default temporary directory
 # (i.e. "/tmp" on *nix systems).
--- a/django/core/exceptions.py
+++ b/django/core/exceptions.py
@@ -55,6 +55,15 @@ class TooManyFieldsSent(SuspiciousOperation):
     pass
 
 
+class TooManyFilesSent(SuspiciousOperation):
+    """
+    The number of fields in a GET or POST request exceeded
+    settings.DATA_UPLOAD_MAX_NUMBER_FILES.
+    """
+
+    pass
+
+
 class RequestDataTooBig(SuspiciousOperation):
     """
     The size of the request (excluding any file uploads) exceeded
--- a/django/core/handlers/exception.py
+++ b/django/core/handlers/exception.py
@@ -6,7 +6,7 @@ from django.conf import settings
 from django.core import signals
 from django.core.exceptions import (
     PermissionDenied, RequestDataTooBig, SuspiciousOperation,
-    TooManyFieldsSent,
+    TooManyFieldsSent, TooManyFilesSent,
 )
 from django.http import Http404
 from django.http.multipartparser import MultiPartParserError
@@ -64,7 +64,7 @@ def response_for_exception(request, exc):
         )
 
     elif isinstance(exc, SuspiciousOperation):
-        if isinstance(exc, (RequestDataTooBig, TooManyFieldsSent)):
+        if isinstance(exc, (RequestDataTooBig, TooManyFieldsSent, TooManyFilesSent)):
             # POST data can't be accessed again, otherwise the original
             # exception would be raised.
             request._mark_post_parse_error()
--- a/django/core/management/commands/flush.py
+++ b/django/core/management/commands/flush.py
@@ -60,7 +60,7 @@ Are you sure you want to do this?
 
         if confirm == 'yes':
             try:
-                connection.ops.execute_sql_flush(database, sql_list)
+                connection.ops.execute_sql_flush(sql_list)
             except Exception as exc:
                 raise CommandError(
                     "Database %s couldn't be flushed. Possible reasons:\n"
--- a/django/core/management/commands/migrate.py
+++ b/django/core/management/commands/migrate.py
@@ -262,33 +262,33 @@ class Command(BaseCommand):
             compute_time = self.verbosity > 1
             if action == "apply_start":
                 if compute_time:
-                    self.start = time.time()
+                    self.start = time.monotonic()
                 self.stdout.write("  Applying %s..." % migration, ending="")
                 self.stdout.flush()
             elif action == "apply_success":
-                elapsed = " (%.3fs)" % (time.time() - self.start) if compute_time else ""
+                elapsed = " (%.3fs)" % (time.monotonic() - self.start) if compute_time else ""
                 if fake:
                     self.stdout.write(self.style.SUCCESS(" FAKED" + elapsed))
                 else:
                     self.stdout.write(self.style.SUCCESS(" OK" + elapsed))
             elif action == "unapply_start":
                 if compute_time:
-                    self.start = time.time()
+                    self.start = time.monotonic()
                 self.stdout.write("  Unapplying %s..." % migration, ending="")
                 self.stdout.flush()
             elif action == "unapply_success":
-                elapsed = " (%.3fs)" % (time.time() - self.start) if compute_time else ""
+                elapsed = " (%.3fs)" % (time.monotonic() - self.start) if compute_time else ""
                 if fake:
                     self.stdout.write(self.style.SUCCESS(" FAKED" + elapsed))
                 else:
                     self.stdout.write(self.style.SUCCESS(" OK" + elapsed))
             elif action == "render_start":
                 if compute_time:
-                    self.start = time.time()
+                    self.start = time.monotonic()
                 self.stdout.write("  Rendering model states...", ending="")
                 self.stdout.flush()
             elif action == "render_success":
-                elapsed = " (%.3fs)" % (time.time() - self.start) if compute_time else ""
+                elapsed = " (%.3fs)" % (time.monotonic() - self.start) if compute_time else ""
                 self.stdout.write(self.style.SUCCESS(" DONE" + elapsed))
 
     def sync_apps(self, connection, app_labels):
--- a/django/core/management/sql.py
+++ b/django/core/management/sql.py
@@ -13,9 +13,12 @@ def sql_flush(style, connection, only_django=False, reset_sequences=True, allow_
         tables = connection.introspection.django_table_names(only_existing=True, include_views=False)
     else:
         tables = connection.introspection.table_names(include_views=False)
-    seqs = connection.introspection.sequence_list() if reset_sequences else ()
-    statements = connection.ops.sql_flush(style, tables, seqs, allow_cascade)
-    return statements
+    return connection.ops.sql_flush(
+        style,
+        tables,
+        reset_sequences=reset_sequences,
+        allow_cascade=allow_cascade,
+    )
 
 
 def emit_pre_migrate_signal(verbosity, interactive, db, **kwargs):
--- a/django/core/validators.py
+++ b/django/core/validators.py
@@ -102,6 +102,7 @@ class URLValidator(RegexValidator):
     message = _('Enter a valid URL.')
     schemes = ['http', 'https', 'ftp', 'ftps']
     unsafe_chars = frozenset('\t\r\n')
+    max_length = 2048
 
     def __init__(self, schemes=None, **kwargs):
         super().__init__(**kwargs)
@@ -109,6 +110,8 @@ class URLValidator(RegexValidator):
             self.schemes = schemes
 
     def __call__(self, value):
+        if isinstance(value, str) and len(value) > self.max_length:
+            raise ValidationError(self.message, code=self.code)
         if isinstance(value, str) and self.unsafe_chars.intersection(value):
             raise ValidationError(self.message, code=self.code)
         # Check if the scheme is valid.
@@ -190,7 +193,9 @@ class EmailValidator:
             self.domain_whitelist = whitelist
 
     def __call__(self, value):
-        if not value or '@' not in value:
+        # The maximum length of an email is 320 characters per RFC 3696
+        # section 3.
+        if not value or '@' not in value or len(value) > 320:
             raise ValidationError(self.message, code=self.code)
 
         user_part, domain_part = value.rsplit('@', 1)
--- a/django/db/backends/base/base.py
+++ b/django/db/backends/base/base.py
@@ -126,8 +126,6 @@ class BaseDatabaseWrapper:
         """
         if not settings.USE_TZ:
             return None
-        elif self.features.supports_timezones:
-            return None
         elif self.settings_dict['TIME_ZONE'] is None:
             return timezone.utc
         else:
@@ -187,7 +185,7 @@ class BaseDatabaseWrapper:
         self.needs_rollback = False
         # Reset parameters defining when to close the connection
         max_age = self.settings_dict['CONN_MAX_AGE']
-        self.close_at = None if max_age is None else time.time() + max_age
+        self.close_at = None if max_age is None else time.monotonic() + max_age
         self.closed_in_transaction = False
         self.errors_occurred = False
         # Establish the connection
@@ -200,15 +198,11 @@ class BaseDatabaseWrapper:
         self.run_on_commit = []
 
     def check_settings(self):
-        if self.settings_dict['TIME_ZONE'] is not None:
-            if not settings.USE_TZ:
-                raise ImproperlyConfigured(
-                    "Connection '%s' cannot set TIME_ZONE because USE_TZ is "
-                    "False." % self.alias)
-            elif self.features.supports_timezones:
-                raise ImproperlyConfigured(
-                    "Connection '%s' cannot set TIME_ZONE because its engine "
-                    "handles time zones conversions natively." % self.alias)
+        if self.settings_dict['TIME_ZONE'] is not None and not settings.USE_TZ:
+            raise ImproperlyConfigured(
+                "Connection '%s' cannot set TIME_ZONE because USE_TZ is False."
+                % self.alias
+            )
 
     def ensure_connection(self):
         """Guarantee that a connection to the database is established."""
@@ -510,7 +504,7 @@ class BaseDatabaseWrapper:
                     self.close()
                     return
 
-            if self.close_at is not None and time.time() >= self.close_at:
+            if self.close_at is not None and time.monotonic() >= self.close_at:
                 self.close()
                 return
 
@@ -617,6 +611,8 @@ class BaseDatabaseWrapper:
         return self.SchemaEditorClass(self, *args, **kwargs)
 
     def on_commit(self, func):
+        if not callable(func):
+            raise TypeError("on_commit()'s callback must be a callable.")
         if self.in_atomic_block:
             # Transaction in progress; save for execution on commit.
             self.run_on_commit.append((set(self.savepoint_ids), func))
--- a/django/db/backends/base/creation.py
+++ b/django/db/backends/base/creation.py
@@ -6,6 +6,7 @@ from django.apps import apps
 from django.conf import settings
 from django.core import serializers
 from django.db import router
+from django.db.transaction import atomic
 
 # The prefix to put on the default database name when creating
 # the test database.
@@ -99,25 +100,25 @@ class BaseDatabaseCreation:
         Designed only for test runner usage; will not handle large
         amounts of data.
         """
-        # Build list of all apps to serialize
-        from django.db.migrations.loader import MigrationLoader
-        loader = MigrationLoader(self.connection)
-        app_list = []
-        for app_config in apps.get_app_configs():
-            if (
-                app_config.models_module is not None and
-                app_config.label in loader.migrated_apps and
-                app_config.name not in settings.TEST_NON_SERIALIZED_APPS
-            ):
-                app_list.append((app_config, None))
-
-        # Make a function to iteratively return every object
+        # Iteratively return every object for all models to serialize.
         def get_objects():
-            for model in serializers.sort_dependencies(app_list):
-                if (model._meta.can_migrate(self.connection) and
-                        router.allow_migrate_model(self.connection.alias, model)):
-                    queryset = model._default_manager.using(self.connection.alias).order_by(model._meta.pk.name)
-                    yield from queryset.iterator()
+            from django.db.migrations.loader import MigrationLoader
+            loader = MigrationLoader(self.connection)
+            for app_config in apps.get_app_configs():
+                if (
+                    app_config.models_module is not None and
+                    app_config.label in loader.migrated_apps and
+                    app_config.name not in settings.TEST_NON_SERIALIZED_APPS
+                ):
+                    for model in app_config.get_models():
+                        if (
+                            model._meta.can_migrate(self.connection) and
+                            router.allow_migrate_model(self.connection.alias, model)
+                        ):
+                            queryset = model._default_manager.using(
+                                self.connection.alias,
+                            ).order_by(model._meta.pk.name)
+                            yield from queryset.iterator()
         # Serialize to a string
         out = StringIO()
         serializers.serialize("json", get_objects(), indent=None, stream=out)
@@ -129,8 +130,18 @@ class BaseDatabaseCreation:
         the serialize_db_to_string() method.
         """
         data = StringIO(data)
-        for obj in serializers.deserialize("json", data, using=self.connection.alias):
-            obj.save()
+        table_names = set()
+        # Load data in a transaction to handle forward references and cycles.
+        with atomic(using=self.connection.alias):
+            # Disable constraint checks, because some databases (MySQL) doesn't
+            # support deferred checks.
+            with self.connection.constraint_checks_disabled():
+                for obj in serializers.deserialize('json', data, using=self.connection.alias):
+                    obj.save()
+                    table_names.add(obj.object.__class__._meta.db_table)
+            # Manually check for any invalid keys that might have been added,
+            # because constraint checks were disabled.
+            self.connection.check_constraints(table_names=table_names)
 
     def _get_database_display_str(self, verbosity, database_name):
         """
--- a/django/db/backends/base/features.py
+++ b/django/db/backends/base/features.py
@@ -36,6 +36,7 @@ class BaseDatabaseFeatures:
     has_select_for_update_nowait = False
     has_select_for_update_skip_locked = False
     has_select_for_update_of = False
+    has_select_for_no_key_update = False
     # Does the database's SELECT FOR UPDATE OF syntax require a column rather
     # than a table?
     select_for_update_of_column = False
@@ -240,6 +241,7 @@ class BaseDatabaseFeatures:
 
     # Does the backend support window expressions (expression OVER (...))?
     supports_over_clause = False
+    only_supports_unbounded_with_preceding_and_following = False
 
     # Does the backend support CAST with precision?
     supports_cast_with_precision = True
@@ -256,9 +258,6 @@ class BaseDatabaseFeatures:
     # Does the backend support keyword parameters for cursor.callproc()?
     supports_callproc_kwargs = False
 
-    # Convert CharField results from bytes to str in database functions.
-    db_functions_convert_bytes_to_str = False
-
     # What formats does the backend EXPLAIN syntax support?
     supported_explain_formats = set()
 
@@ -306,3 +305,8 @@ class BaseDatabaseFeatures:
             count, = cursor.fetchone()
             cursor.execute('DROP TABLE ROLLBACK_TEST')
         return count == 0
+
+    def allows_group_by_selected_pks_on_model(self, model):
+        if not self.allows_group_by_selected_pks:
+            return False
+        return model._meta.managed
--- a/django/db/backends/base/introspection.py
+++ b/django/db/backends/base/introspection.py
@@ -54,6 +54,16 @@ class BaseDatabaseIntrospection:
         """
         raise NotImplementedError('subclasses of BaseDatabaseIntrospection may require a get_table_list() method')
 
+    def get_migratable_models(self):
+        from django.apps import apps
+        from django.db import router
+        return (
+            model
+            for app_config in apps.get_app_configs()
+            for model in router.get_migratable_models(app_config, self.connection.alias)
+            if model._meta.can_migrate(self.connection)
+        )
+
     def django_table_names(self, only_existing=False, include_views=True):
         """
         Return a list of all table names that have associated Django models and
@@ -61,18 +71,15 @@ class BaseDatabaseIntrospection:
 
         If only_existing is True, include only the tables in the database.
         """
-        from django.apps import apps
-        from django.db import router
         tables = set()
-        for app_config in apps.get_app_configs():
-            for model in router.get_migratable_models(app_config, self.connection.alias):
-                if not model._meta.managed:
-                    continue
-                tables.add(model._meta.db_table)
-                tables.update(
-                    f.m2m_db_table() for f in model._meta.local_many_to_many
-                    if f.remote_field.through._meta.managed
-                )
+        for model in self.get_migratable_models():
+            if not model._meta.managed:
+                continue
+            tables.add(model._meta.db_table)
+            tables.update(
+                f.m2m_db_table() for f in model._meta.local_many_to_many
+                if f.remote_field.through._meta.managed
+            )
         tables = list(tables)
         if only_existing:
             existing_tables = set(self.table_names(include_views=include_views))
@@ -88,14 +95,9 @@ class BaseDatabaseIntrospection:
         Return a set of all models represented by the provided list of table
         names.
         """
-        from django.apps import apps
-        from django.db import router
-        all_models = []
-        for app_config in apps.get_app_configs():
-            all_models.extend(router.get_migratable_models(app_config, self.connection.alias))
         tables = set(map(self.identifier_converter, tables))
         return {
-            m for m in all_models
+            m for m in self.get_migratable_models
             if self.identifier_converter(m._meta.db_table) in tables
         }
 
@@ -104,24 +106,21 @@ class BaseDatabaseIntrospection:
         Return a list of information about all DB sequences for all models in
         all apps.
         """
-        from django.apps import apps
-        from django.db import router
 
         sequence_list = []
         with self.connection.cursor() as cursor:
-            for app_config in apps.get_app_configs():
-                for model in router.get_migratable_models(app_config, self.connection.alias):
-                    if not model._meta.managed:
-                        continue
-                    if model._meta.swapped:
-                        continue
-                    sequence_list.extend(self.get_sequences(cursor, model._meta.db_table, model._meta.local_fields))
-                    for f in model._meta.local_many_to_many:
-                        # If this is an m2m using an intermediate table,
-                        # we don't need to reset the sequence.
-                        if f.remote_field.through._meta.auto_created:
-                            sequence = self.get_sequences(cursor, f.m2m_db_table())
-                            sequence_list.extend(sequence or [{'table': f.m2m_db_table(), 'column': None}])
+            for model in self.get_migratable_models():
+                if not model._meta.managed:
+                    continue
+                if model._meta.swapped:
+                    continue
+                sequence_list.extend(self.get_sequences(cursor, model._meta.db_table, model._meta.local_fields))
+                for f in model._meta.local_many_to_many:
+                    # If this is an m2m using an intermediate table,
+                    # we don't need to reset the sequence.
+                    if f.remote_field.through._meta.auto_created:
+                        sequence = self.get_sequences(cursor, f.m2m_db_table())
+                        sequence_list.extend(sequence or [{'table': f.m2m_db_table(), 'column': None}])
         return sequence_list
 
     def get_sequences(self, cursor, table_name, table_fields=()):
--- a/django/db/backends/base/operations.py
+++ b/django/db/backends/base/operations.py
@@ -9,6 +9,7 @@ from django.db import NotSupportedError, transaction
 from django.db.backends import utils
 from django.utils import timezone
 from django.utils.encoding import force_text
+from django.utils.regex_helper import _lazy_re_compile
 
 
 class BaseDatabaseOperations:
@@ -26,6 +27,8 @@ class BaseDatabaseOperations:
         'BigIntegerField': (-9223372036854775808, 9223372036854775807),
         'PositiveSmallIntegerField': (0, 32767),
         'PositiveIntegerField': (0, 2147483647),
+        'AutoField': (-2147483648, 2147483647),
+        'BigAutoField': (-9223372036854775808, 9223372036854775807),
     }
     set_operators = {
         'union': 'UNION',
@@ -49,6 +52,8 @@ class BaseDatabaseOperations:
     # Prefix for EXPLAIN queries, or None EXPLAIN isn't supported.
     explain_prefix = None
 
+    extract_trunc_lookup_pattern = _lazy_re_compile(r"[\w\-_()]+")
+
     def __init__(self, connection):
         self.connection = connection
         self._cache = None
@@ -101,7 +106,7 @@ class BaseDatabaseOperations:
         """
         raise NotImplementedError('subclasses of BaseDatabaseOperations may require a date_interval_sql() method')
 
-    def date_trunc_sql(self, lookup_type, field_name):
+    def date_trunc_sql(self, lookup_type, field_name, tzname=None):
         """
         Given a lookup_type of 'year', 'month', or 'day', return the SQL that
         truncates the given date field field_name to a date object with only
@@ -140,7 +145,7 @@ class BaseDatabaseOperations:
         """
         raise NotImplementedError('subclasses of BaseDatabaseOperations may require a datetime_trunc_sql() method')
 
-    def time_trunc_sql(self, lookup_type, field_name):
+    def time_trunc_sql(self, lookup_type, field_name, tzname=None):
         """
         Given a lookup_type of 'hour', 'minute' or 'second', return the SQL
         that truncates the given time field field_name to a time object with
@@ -197,11 +202,12 @@ class BaseDatabaseOperations:
         """
         return []
 
-    def for_update_sql(self, nowait=False, skip_locked=False, of=()):
+    def for_update_sql(self, nowait=False, skip_locked=False, of=(), no_key=False):
         """
         Return the FOR UPDATE SQL clause to lock rows for an update operation.
         """
-        return 'FOR UPDATE%s%s%s' % (
+        return 'FOR%s UPDATE%s%s%s' % (
+            ' NO KEY' if no_key else '',
             ' OF %s' % ', '.join(of) if of else '',
             ' NOWAIT' if nowait else '',
             ' SKIP LOCKED' if skip_locked else '',
@@ -218,10 +224,10 @@ class BaseDatabaseOperations:
     def limit_offset_sql(self, low_mark, high_mark):
         """Return LIMIT/OFFSET SQL clause."""
         limit, offset = self._get_limit_offset_params(low_mark, high_mark)
-        return '%s%s' % (
-            (' LIMIT %d' % limit) if limit else '',
-            (' OFFSET %d' % offset) if offset else '',
-        )
+        return ' '.join(sql for sql in (
+            ('LIMIT %d' % limit) if limit else None,
+            ('OFFSET %d' % offset) if offset else None,
+        ) if sql)
 
     def last_executed_query(self, cursor, sql, params):
         """
@@ -380,7 +386,7 @@ class BaseDatabaseOperations:
         """
         return ''
 
-    def sql_flush(self, style, tables, sequences, allow_cascade=False):
+    def sql_flush(self, style, tables, *, reset_sequences=False, allow_cascade=False):
         """
         Return a list of SQL statements required to remove all data from
         the given database tables (without actually removing the tables
@@ -396,9 +402,12 @@ class BaseDatabaseOperations:
         """
         raise NotImplementedError('subclasses of BaseDatabaseOperations must provide an sql_flush() method')
 
-    def execute_sql_flush(self, using, sql_list):
+    def execute_sql_flush(self, sql_list):
         """Execute a list of SQL statements to flush the database."""
-        with transaction.atomic(using=using, savepoint=self.connection.features.can_rollback_ddl):
+        with transaction.atomic(
+            using=self.connection.alias,
+            savepoint=self.connection.features.can_rollback_ddl,
+        ):
             with self.connection.cursor() as cursor:
                 for sql in sql_list:
                     cursor.execute(sql)
@@ -617,7 +626,7 @@ class BaseDatabaseOperations:
         if self.connection.features.supports_temporal_subtraction:
             lhs_sql, lhs_params = lhs
             rhs_sql, rhs_params = rhs
-            return "(%s - %s)" % (lhs_sql, rhs_sql), lhs_params + rhs_params
+            return '(%s - %s)' % (lhs_sql, rhs_sql), (*lhs_params, *rhs_params)
         raise NotSupportedError("This backend does not support %s subtraction." % internal_type)
 
     def window_frame_start(self, start):
@@ -649,7 +658,16 @@ class BaseDatabaseOperations:
         return self.window_frame_start(start), self.window_frame_end(end)
 
     def window_frame_range_start_end(self, start=None, end=None):
-        return self.window_frame_rows_start_end(start, end)
+        start_, end_ = self.window_frame_rows_start_end(start, end)
+        if (
+            self.connection.features.only_supports_unbounded_with_preceding_and_following and
+            ((start and start < 0) or (end and end > 0))
+        ):
+            raise NotSupportedError(
+                '%s only supports UNBOUNDED together with PRECEDING and '
+                'FOLLOWING.' % self.connection.display_name
+            )
+        return start_, end_
 
     def explain_query_prefix(self, format=None, **options):
         if not self.connection.features.supports_explaining_query_execution:
--- a/django/db/backends/base/schema.py
+++ b/django/db/backends/base/schema.py
@@ -212,9 +212,9 @@ class BaseDatabaseSchemaEditor:
             default = field.get_default()
         elif not field.null and field.blank and field.empty_strings_allowed:
             if field.get_internal_type() == "BinaryField":
-                default = bytes()
+                default = b''
             else:
-                default = str()
+                default = ''
         elif getattr(field, 'auto_now', False) or getattr(field, 'auto_now_add', False):
             default = datetime.now()
             internal_type = field.get_internal_type()
@@ -342,7 +342,7 @@ class BaseDatabaseSchemaEditor:
         """Add a check constraint to a model."""
         sql = constraint.create_sql(model, self)
         if sql:
-            self.execute(sql)
+            self.execute(sql, params=None)
 
     def remove_constraint(self, model, constraint):
         """Remove a check constraint from a model."""
@@ -376,7 +376,12 @@ class BaseDatabaseSchemaEditor:
         news = {tuple(fields) for fields in new_index_together}
         # Deleted indexes
         for fields in olds.difference(news):
-            self._delete_composed_index(model, fields, {'index': True}, self.sql_delete_index)
+            self._delete_composed_index(
+                model,
+                fields,
+                {'index': True, 'unique': False},
+                self.sql_delete_index,
+            )
         # Created indexes
         for field_names in news.difference(olds):
             fields = [model._meta.get_field(field) for field in field_names]
@@ -501,6 +506,8 @@ class BaseDatabaseSchemaEditor:
         If `strict` is True, raise errors if the old column does not match
         `old_field` precisely.
         """
+        if not self._field_should_be_altered(old_field, new_field):
+            return
         # Ensure this field is even column-based
         old_db_params = old_field.db_parameters(connection=self.connection)
         old_type = old_db_params['type']
@@ -645,17 +652,17 @@ class BaseDatabaseSchemaEditor:
         #  3. Replace NULL constraint with NOT NULL
         #  4. Drop the default again.
         # Default change?
-        old_default = self.effective_default(old_field)
-        new_default = self.effective_default(new_field)
-        needs_database_default = (
-            old_field.null and
-            not new_field.null and
-            old_default != new_default and
-            new_default is not None and
-            not self.skip_default(new_field)
-        )
-        if needs_database_default:
-            actions.append(self._alter_column_default_sql(model, old_field, new_field))
+        needs_database_default = False
+        if old_field.null and not new_field.null:
+            old_default = self.effective_default(old_field)
+            new_default = self.effective_default(new_field)
+            if (
+                not self.skip_default(new_field) and
+                old_default != new_default and
+                new_default is not None
+            ):
+                needs_database_default = True
+                actions.append(self._alter_column_default_sql(model, old_field, new_field))
         # Nullability change?
         if old_field.null != new_field.null:
             fragment = self._alter_column_null_sql(model, old_field, new_field)
@@ -912,6 +919,11 @@ class BaseDatabaseSchemaEditor:
             return ' ' + self.connection.ops.tablespace_sql(db_tablespace)
         return ''
 
+    def _index_condition_sql(self, condition):
+        if condition:
+            return ' WHERE ' + condition
+        return ''
+
     def _create_index_sql(self, model, fields, *, name=None, suffix='', using='',
                           db_tablespace=None, col_suffixes=(), sql=None, opclasses=(),
                           condition=None):
@@ -938,7 +950,7 @@ class BaseDatabaseSchemaEditor:
             using=using,
             columns=self._index_columns(table, columns, col_suffixes, opclasses),
             extra=tablespace_sql,
-            condition=(' WHERE ' + condition) if condition else '',
+            condition=self._index_condition_sql(condition),
         )
 
     def _delete_index_sql(self, model, name):
@@ -979,6 +991,35 @@ class BaseDatabaseSchemaEditor:
             output.append(self._create_index_sql(model, [field]))
         return output
 
+    def _field_should_be_altered(self, old_field, new_field):
+        _, old_path, old_args, old_kwargs = old_field.deconstruct()
+        _, new_path, new_args, new_kwargs = new_field.deconstruct()
+        # Don't alter when:
+        # - changing only a field name
+        # - changing an attribute that doesn't affect the schema
+        # - adding only a db_column and the column name is not changed
+        non_database_attrs = [
+            'blank',
+            'db_column',
+            'editable',
+            'error_messages',
+            'help_text',
+            'limit_choices_to',
+            # Database-level options are not supported, see #21961.
+            'on_delete',
+            'related_name',
+            'related_query_name',
+            'validators',
+            'verbose_name',
+        ]
+        for attr in non_database_attrs:
+            old_kwargs.pop(attr, None)
+            new_kwargs.pop(attr, None)
+        return (
+            self.quote_name(old_field.column) != self.quote_name(new_field.column) or
+            (old_path, old_args, old_kwargs) != (new_path, new_args, new_kwargs)
+        )
+
     def _field_should_be_indexed(self, model, field):
         return field.db_index and not field.unique
 
@@ -1055,20 +1096,18 @@ class BaseDatabaseSchemaEditor:
             name = self.quote_name(name)
         columns = Columns(table, columns, self.quote_name)
         if condition:
-            return Statement(
-                self.sql_create_unique_index,
-                table=table,
-                name=name,
-                columns=columns,
-                condition=' WHERE ' + condition,
-            ) if self.connection.features.supports_partial_indexes else None
+            if not self.connection.features.supports_partial_indexes:
+                return None
+            sql = self.sql_create_unique_index
         else:
-            return Statement(
-                self.sql_create_unique,
-                table=table,
-                name=name,
-                columns=columns,
-            )
+            sql = self.sql_create_unique
+        return Statement(
+            sql,
+            table=table,
+            name=name,
+            columns=columns,
+            condition=self._index_condition_sql(condition),
+        )
 
     def _delete_unique_sql(self, model, name, condition=None):
         if condition:
--- a/django/db/backends/ddl_references.py
+++ b/django/db/backends/ddl_references.py
@@ -83,10 +83,14 @@ class Columns(TableColumns):
 
     def __str__(self):
         def col_str(column, idx):
+            col = self.quote_name(column)
             try:
-                return self.quote_name(column) + self.col_suffixes[idx]
+                suffix = self.col_suffixes[idx]
+                if suffix:
+                    col = '{} {}'.format(col, suffix)
             except IndexError:
-                return self.quote_name(column)
+                pass
+            return col
 
         return ', '.join(col_str(column, idx) for idx, column in enumerate(self.columns))
 
@@ -114,7 +118,9 @@ class IndexColumns(Columns):
             # length as self.columns.
             col = '{} {}'.format(self.quote_name(column), self.opclasses[idx])
             try:
-                col = '{} {}'.format(col, self.col_suffixes[idx])
+                suffix = self.col_suffixes[idx]
+                if suffix:
+                    col = '{} {}'.format(col, suffix)
             except IndexError:
                 pass
             return col
--- a/django/db/backends/postgresql/base.py
+++ b/django/db/backends/postgresql/base.py
@@ -11,6 +11,9 @@ from django.conf import settings
 from django.core.exceptions import ImproperlyConfigured
 from django.db import connections
 from django.db.backends.base.base import BaseDatabaseWrapper
+from django.db.backends.utils import (
+    CursorDebugWrapper as BaseCursorDebugWrapper,
+)
 from django.db.utils import DatabaseError as WrappedDatabaseError
 from django.utils.functional import cached_property
 from django.utils.safestring import SafeText
@@ -42,7 +45,6 @@ from .features import DatabaseFeatures                      # NOQA isort:skip
 from .introspection import DatabaseIntrospection            # NOQA isort:skip
 from .operations import DatabaseOperations                  # NOQA isort:skip
 from .schema import DatabaseSchemaEditor                    # NOQA isort:skip
-from .utils import utc_tzinfo_factory                       # NOQA isort:skip
 
 psycopg2.extensions.register_adapter(SafeText, psycopg2.extensions.QuotedString)
 psycopg2.extras.register_uuid()
@@ -192,6 +194,10 @@ class DatabaseWrapper(BaseDatabaseWrapper):
             if self.isolation_level != connection.isolation_level:
                 connection.set_session(isolation_level=self.isolation_level)
 
+        # Register dummy loads() to avoid a round trip from psycopg2's decode
+        # to json.dumps() to json.loads(), when using a custom decoder in
+        # JSONField.
+        psycopg2.extras.register_default_jsonb(conn_or_curs=connection, loads=lambda x: x)
         return connection
 
     def ensure_timezone(self):
@@ -221,9 +227,12 @@ class DatabaseWrapper(BaseDatabaseWrapper):
             cursor = self.connection.cursor(name, scrollable=False, withhold=self.connection.autocommit)
         else:
             cursor = self.connection.cursor()
-        cursor.tzinfo_factory = utc_tzinfo_factory if settings.USE_TZ else None
+        cursor.tzinfo_factory = self.tzinfo_factory if settings.USE_TZ else None
         return cursor
 
+    def tzinfo_factory(self, offset):
+        return self.timezone
+
     def chunked_cursor(self):
         self._named_cursor_idx += 1
         return self._cursor(
@@ -243,13 +252,15 @@ class DatabaseWrapper(BaseDatabaseWrapper):
         Check constraints by setting them to immediate. Return them to deferred
         afterward.
         """
-        self.cursor().execute('SET CONSTRAINTS ALL IMMEDIATE')
-        self.cursor().execute('SET CONSTRAINTS ALL DEFERRED')
+        with self.cursor() as cursor:
+            cursor.execute('SET CONSTRAINTS ALL IMMEDIATE')
+            cursor.execute('SET CONSTRAINTS ALL DEFERRED')
 
     def is_usable(self):
         try:
             # Use a psycopg cursor directly, bypassing Django's utilities.
-            self.connection.cursor().execute("SELECT 1")
+            with self.connection.cursor() as cursor:
+                cursor.execute('SELECT 1')
         except Database.Error:
             return False
         else:
@@ -281,3 +292,16 @@ class DatabaseWrapper(BaseDatabaseWrapper):
     def pg_version(self):
         with self.temporary_connection():
             return self.connection.server_version
+
+    def make_debug_cursor(self, cursor):
+        return CursorDebugWrapper(cursor, self)
+
+
+class CursorDebugWrapper(BaseCursorDebugWrapper):
+    def copy_expert(self, sql, file, *args):
+        with self.debug_sql(sql):
+            return self.cursor.copy_expert(sql, file, *args)
+
+    def copy_to(self, file, table, *args, **kwargs):
+        with self.debug_sql(sql='COPY %s TO STDOUT' % table):
+            return self.cursor.copy_to(file, table, *args, **kwargs)
--- a/django/db/backends/postgresql/client.py
+++ b/django/db/backends/postgresql/client.py
@@ -2,17 +2,9 @@ import os
 import signal
 import subprocess
 
-from django.core.files.temp import NamedTemporaryFile
 from django.db.backends.base.client import BaseDatabaseClient
 
 
-def _escape_pgpass(txt):
-    """
-    Escape a fragment of a PostgreSQL .pgpass file.
-    """
-    return txt.replace('\\', '\\\\').replace(':', '\\:')
-
-
 class DatabaseClient(BaseDatabaseClient):
     executable_name = 'psql'
 
@@ -34,38 +26,17 @@ class DatabaseClient(BaseDatabaseClient):
             args += ['-p', str(port)]
         args += [dbname]
 
-        temp_pgpass = None
         sigint_handler = signal.getsignal(signal.SIGINT)
+        subprocess_env = os.environ.copy()
+        if passwd:
+            subprocess_env['PGPASSWORD'] = str(passwd)
         try:
-            if passwd:
-                # Create temporary .pgpass file.
-                temp_pgpass = NamedTemporaryFile(mode='w+')
-                try:
-                    print(
-                        _escape_pgpass(host) or '*',
-                        str(port) or '*',
-                        _escape_pgpass(dbname) or '*',
-                        _escape_pgpass(user) or '*',
-                        _escape_pgpass(passwd),
-                        file=temp_pgpass,
-                        sep=':',
-                        flush=True,
-                    )
-                    os.environ['PGPASSFILE'] = temp_pgpass.name
-                except UnicodeEncodeError:
-                    # If the current locale can't encode the data, let the
-                    # user input the password manually.
-                    pass
             # Allow SIGINT to pass to psql to abort queries.
             signal.signal(signal.SIGINT, signal.SIG_IGN)
-            subprocess.check_call(args)
+            subprocess.run(args, check=True, env=subprocess_env)
         finally:
             # Restore the original SIGINT handler.
             signal.signal(signal.SIGINT, sigint_handler)
-            if temp_pgpass:
-                temp_pgpass.close()
-                if 'PGPASSFILE' in os.environ:  # unit tests need cleanup
-                    del os.environ['PGPASSFILE']
 
     def runshell(self):
         DatabaseClient.runshell_db(self.connection.get_connection_params())
--- a/django/db/backends/postgresql/creation.py
+++ b/django/db/backends/postgresql/creation.py
@@ -48,7 +48,7 @@ class DatabaseCreation(BaseDatabaseCreation):
             elif not keepdb:
                 # If the database should be kept, ignore "database already
                 # exists".
-                raise e
+                raise
 
     def _clone_test_db(self, suffix, verbosity, keepdb=False):
         # CREATE DATABASE ... WITH TEMPLATE ... requires closing connections
--- a/django/db/backends/postgresql/features.py
+++ b/django/db/backends/postgresql/features.py
@@ -16,6 +16,12 @@ class DatabaseFeatures(BaseDatabaseFeatures):
     has_select_for_update = True
     has_select_for_update_nowait = True
     has_select_for_update_of = True
+    has_select_for_update_skip_locked = True
+    has_select_for_no_key_update = True
+    has_brin_index_support = True
+    has_jsonb_agg = True
+    has_gin_pending_list_limit = True
+    supports_ignore_conflicts = True
     can_release_savepoints = True
     supports_tablespaces = True
     supports_transactions = True
@@ -51,26 +57,13 @@ class DatabaseFeatures(BaseDatabaseFeatures):
     $$ LANGUAGE plpgsql;"""
     requires_casted_case_in_updates = True
     supports_over_clause = True
+    only_supports_unbounded_with_preceding_and_following = True
     supports_aggregate_filter_clause = True
     supported_explain_formats = {'JSON', 'TEXT', 'XML', 'YAML'}
 
-    @cached_property
-    def is_postgresql_9_5(self):
-        return self.connection.pg_version >= 90500
-
-    @cached_property
-    def is_postgresql_9_6(self):
-        return self.connection.pg_version >= 90600
-
     @cached_property
     def is_postgresql_10(self):
         return self.connection.pg_version >= 100000
 
-    has_select_for_update_skip_locked = property(operator.attrgetter('is_postgresql_9_5'))
-    has_brin_index_support = property(operator.attrgetter('is_postgresql_9_5'))
-    has_jsonb_agg = property(operator.attrgetter('is_postgresql_9_5'))
     has_brin_autosummarize = property(operator.attrgetter('is_postgresql_10'))
-    has_gin_pending_list_limit = property(operator.attrgetter('is_postgresql_9_5'))
-    supports_ignore_conflicts = property(operator.attrgetter('is_postgresql_9_5'))
-    has_phraseto_tsquery = property(operator.attrgetter('is_postgresql_9_6'))
     supports_table_partitions = property(operator.attrgetter('is_postgresql_10'))
--- a/django/db/backends/postgresql/introspection.py
+++ b/django/db/backends/postgresql/introspection.py
@@ -27,6 +27,8 @@ class DatabaseIntrospection(BaseDatabaseIntrospection):
         1700: 'DecimalField',
         2950: 'UUIDField',
     }
+    # A hook for subclasses.
+    index_default_access_method = 'btree'
 
     ignored_tables = []
 
@@ -99,10 +101,9 @@ class DatabaseIntrospection(BaseDatabaseIntrospection):
                 JOIN pg_attrdef ad ON ad.oid = d.objid AND d.classid = 'pg_attrdef'::regclass
                 JOIN pg_attribute col ON col.attrelid = ad.adrelid AND col.attnum = ad.adnum
                 JOIN pg_class tbl ON tbl.oid = ad.adrelid
-                JOIN pg_namespace n ON n.oid = tbl.relnamespace
             WHERE s.relkind = 'S'
               AND d.deptype in ('a', 'n')
-              AND n.nspname = 'public'
+              AND pg_catalog.pg_table_is_visible(tbl.oid)
               AND tbl.relname = %s
         """, [table_name])
         return [
@@ -115,30 +116,21 @@ class DatabaseIntrospection(BaseDatabaseIntrospection):
         Return a dictionary of {field_name: (field_name_other_table, other_table)}
         representing all relationships to the given table.
         """
+        return {row[0]: (row[2], row[1]) for row in self.get_key_columns(cursor, table_name)}
+
+    def get_key_columns(self, cursor, table_name):
         cursor.execute("""
-            SELECT c2.relname, a1.attname, a2.attname
+            SELECT a1.attname, c2.relname, a2.attname
             FROM pg_constraint con
             LEFT JOIN pg_class c1 ON con.conrelid = c1.oid
             LEFT JOIN pg_class c2 ON con.confrelid = c2.oid
             LEFT JOIN pg_attribute a1 ON c1.oid = a1.attrelid AND a1.attnum = con.conkey[1]
             LEFT JOIN pg_attribute a2 ON c2.oid = a2.attrelid AND a2.attnum = con.confkey[1]
-            WHERE c1.relname = %s AND con.contype = 'f'
-        """, [table_name])
-        return {row[1]: (row[2], row[0]) for row in cursor.fetchall()}
-
-    def get_key_columns(self, cursor, table_name):
-        cursor.execute("""
-            SELECT kcu.column_name, ccu.table_name AS referenced_table, ccu.column_name AS referenced_column
-            FROM information_schema.constraint_column_usage ccu
-            LEFT JOIN information_schema.key_column_usage kcu
-                ON ccu.constraint_catalog = kcu.constraint_catalog
-                    AND ccu.constraint_schema = kcu.constraint_schema
-                    AND ccu.constraint_name = kcu.constraint_name
-            LEFT JOIN information_schema.table_constraints tc
-                ON ccu.constraint_catalog = tc.constraint_catalog
-                    AND ccu.constraint_schema = tc.constraint_schema
-                    AND ccu.constraint_name = tc.constraint_name
-            WHERE kcu.table_name = %s AND tc.constraint_type = 'FOREIGN KEY'
+            WHERE
+                c1.relname = %s AND
+                con.contype = 'f' AND
+                c1.relnamespace = c2.relnamespace AND
+                pg_catalog.pg_table_is_visible(c1.oid)
         """, [table_name])
         return cursor.fetchall()
 
@@ -170,9 +162,8 @@ class DatabaseIntrospection(BaseDatabaseIntrospection):
                 cl.reloptions
             FROM pg_constraint AS c
             JOIN pg_class AS cl ON c.conrelid = cl.oid
-            JOIN pg_namespace AS ns ON cl.relnamespace = ns.oid
-            WHERE ns.nspname = %s AND cl.relname = %s
-        """, ["public", table_name])
+            WHERE cl.relname = %s AND pg_catalog.pg_table_is_visible(cl.oid)
+        """, [table_name])
         for constraint, columns, kind, used_cols, options in cursor.fetchall():
             constraints[constraint] = {
                 "columns": columns,
@@ -197,7 +188,7 @@ class DatabaseIntrospection(BaseDatabaseIntrospection):
                             pg_get_indexdef(idx.indexrelid)
                     END AS exprdef,
                     CASE am.amname
-                        WHEN 'btree' THEN
+                        WHEN %s THEN
                             CASE (option & 1)
                                 WHEN 1 THEN 'DESC' ELSE 'ASC'
                             END
@@ -211,13 +202,18 @@ class DatabaseIntrospection(BaseDatabaseIntrospection):
                 LEFT JOIN pg_class c2 ON idx.indexrelid = c2.oid
                 LEFT JOIN pg_am am ON c2.relam = am.oid
                 LEFT JOIN pg_attribute attr ON attr.attrelid = c.oid AND attr.attnum = idx.key
-                WHERE c.relname = %s
+                WHERE c.relname = %s AND pg_catalog.pg_table_is_visible(c.oid)
             ) s2
             GROUP BY indexname, indisunique, indisprimary, amname, exprdef, attoptions;
-        """, [table_name])
+        """, [self.index_default_access_method, table_name])
         for index, columns, unique, primary, orders, type_, definition, options in cursor.fetchall():
             if index not in constraints:
-                basic_index = type_ == 'btree' and not index.endswith('_btree') and options is None
+                basic_index = (
+                    type_ == self.index_default_access_method and
+                    # '_btree' references
+                    # django.contrib.postgres.indexes.BTreeIndex.suffix.
+                    not index.endswith('_btree') and options is None
+                )
                 constraints[index] = {
                     "columns": columns if columns != [None] else [],
                     "orders": orders if orders != [None] else [],
--- a/django/db/backends/postgresql/operations.py
+++ b/django/db/backends/postgresql/operations.py
@@ -1,7 +1,6 @@
 from psycopg2.extras import Inet
 
 from django.conf import settings
-from django.db import NotSupportedError
 from django.db.backends.base.operations import BaseDatabaseOperations
 
 
@@ -48,13 +47,21 @@ class DatabaseOperations(BaseDatabaseOperations):
         else:
             return "EXTRACT('%s' FROM %s)" % (lookup_type, field_name)
 
-    def date_trunc_sql(self, lookup_type, field_name):
+    def date_trunc_sql(self, lookup_type, field_name, tzname=None):
+        field_name = self._convert_field_to_tz(field_name, tzname)
         # https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC
         return "DATE_TRUNC('%s', %s)" % (lookup_type, field_name)
 
+    def _prepare_tzname_delta(self, tzname):
+        if '+' in tzname:
+            return tzname.replace('+', '-')
+        elif '-' in tzname:
+            return tzname.replace('-', '+')
+        return tzname
+
     def _convert_field_to_tz(self, field_name, tzname):
-        if settings.USE_TZ:
-            field_name = "%s AT TIME ZONE '%s'" % (field_name, tzname)
+        if tzname and settings.USE_TZ:
+            field_name = "%s AT TIME ZONE '%s'" % (field_name, self._prepare_tzname_delta(tzname))
         return field_name
 
     def datetime_cast_date_sql(self, field_name, tzname):
@@ -74,7 +81,8 @@ class DatabaseOperations(BaseDatabaseOperations):
         # https://www.postgresql.org/docs/current/functions-datetime.html#FUNCTIONS-DATETIME-TRUNC
         return "DATE_TRUNC('%s', %s)" % (lookup_type, field_name)
 
-    def time_trunc_sql(self, lookup_type, field_name):
+    def time_trunc_sql(self, lookup_type, field_name, tzname=None):
+        field_name = self._convert_field_to_tz(field_name, tzname)
         return "DATE_TRUNC('%s', %s)::time" % (lookup_type, field_name)
 
     def deferrable_sql(self):
@@ -121,29 +129,22 @@ class DatabaseOperations(BaseDatabaseOperations):
     def set_time_zone_sql(self):
         return "SET TIME ZONE %s"
 
-    def sql_flush(self, style, tables, sequences, allow_cascade=False):
-        if tables:
-            # Perform a single SQL 'TRUNCATE x, y, z...;' statement.  It allows
-            # us to truncate tables referenced by a foreign key in any other
-            # table.
-            tables_sql = ', '.join(
-                style.SQL_FIELD(self.quote_name(table)) for table in tables)
-            if allow_cascade:
-                sql = ['%s %s %s;' % (
-                    style.SQL_KEYWORD('TRUNCATE'),
-                    tables_sql,
-                    style.SQL_KEYWORD('CASCADE'),
-                )]
-            else:
-                sql = ['%s %s;' % (
-                    style.SQL_KEYWORD('TRUNCATE'),
-                    tables_sql,
-                )]
-            sql.extend(self.sequence_reset_by_name_sql(style, sequences))
-            return sql
-        else:
+    def sql_flush(self, style, tables, *, reset_sequences=False, allow_cascade=False):
+        if not tables:
             return []
 
+        # Perform a single SQL 'TRUNCATE x, y, z...;' statement. It allows us
+        # to truncate tables referenced by a foreign key in any other table.
+        sql_parts = [
+            style.SQL_KEYWORD('TRUNCATE'),
+            ', '.join(style.SQL_FIELD(self.quote_name(table)) for table in tables),
+        ]
+        if reset_sequences:
+            sql_parts.append(style.SQL_KEYWORD('RESTART IDENTITY'))
+        if allow_cascade:
+            sql_parts.append(style.SQL_KEYWORD('CASCADE'))
+        return ['%s;' % ' '.join(sql_parts)]
+
     def sequence_reset_by_name_sql(self, style, sequences):
         # 'ALTER SEQUENCE sequence_name RESTART WITH 1;'... style SQL statements
         # to reset sequence indices
@@ -257,6 +258,9 @@ class DatabaseOperations(BaseDatabaseOperations):
     def adapt_timefield_value(self, value):
         return value
 
+    def adapt_decimalfield_value(self, value, max_digits=None, decimal_places=None):
+        return value
+
     def adapt_ipaddressfield_value(self, value):
         if value:
             return Inet(value)
@@ -266,18 +270,10 @@ class DatabaseOperations(BaseDatabaseOperations):
         if internal_type == 'DateField':
             lhs_sql, lhs_params = lhs
             rhs_sql, rhs_params = rhs
-            return "(interval '1 day' * (%s - %s))" % (lhs_sql, rhs_sql), lhs_params + rhs_params
+            params = (*lhs_params, *rhs_params)
+            return "(interval '1 day' * (%s - %s))" % (lhs_sql, rhs_sql), params
         return super().subtract_temporals(internal_type, lhs, rhs)
 
-    def window_frame_range_start_end(self, start=None, end=None):
-        start_, end_ = super().window_frame_range_start_end(start, end)
-        if (start and start < 0) or (end and end > 0):
-            raise NotSupportedError(
-                'PostgreSQL only supports UNBOUNDED together with PRECEDING '
-                'and FOLLOWING.'
-            )
-        return start_, end_
-
     def explain_query_prefix(self, format=None, **options):
         extra = {}
         # Normalize options.
--- a/django/db/backends/postgresql/schema.py
+++ b/django/db/backends/postgresql/schema.py
@@ -2,15 +2,15 @@ import psycopg2
 
 from django.db.backends.base.schema import BaseDatabaseSchemaEditor
 from django.db.backends.ddl_references import IndexColumns
+from django.db.backends.utils import strip_quotes
 
 
 class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):
 
-    sql_alter_column_type = "ALTER COLUMN %(column)s TYPE %(type)s USING %(column)s::%(type)s"
-
     sql_create_sequence = "CREATE SEQUENCE %(sequence)s"
     sql_delete_sequence = "DROP SEQUENCE IF EXISTS %(sequence)s CASCADE"
     sql_set_sequence_max = "SELECT setval('%(sequence)s', MAX(%(column)s)) FROM %(table)s"
+    sql_set_sequence_owner = 'ALTER SEQUENCE %(sequence)s OWNED BY %(table)s.%(column)s'
 
     sql_create_index = "CREATE INDEX %(name)s ON %(table)s%(using)s (%(columns)s)%(extra)s%(condition)s"
     sql_delete_index = "DROP INDEX IF EXISTS %(name)s"
@@ -24,8 +24,11 @@ class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):
     def quote_value(self, value):
         if isinstance(value, str):
             value = value.replace('%', '%%')
+        adapted = psycopg2.extensions.adapt(value)
+        if hasattr(adapted, 'encoding'):
+            adapted.encoding = 'utf8'
         # getquoted() returns a quoted bytestring of the adapted value.
-        return psycopg2.extensions.adapt(value).getquoted().decode()
+        return adapted.getquoted().decode()
 
     def _field_indexes_sql(self, model, field):
         output = super()._field_indexes_sql(model, field)
@@ -34,6 +37,21 @@ class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):
             output.append(like_index_statement)
         return output
 
+    def _field_data_type(self, field):
+        if field.is_relation:
+            return field.rel_db_type(self.connection)
+        return self.connection.data_types.get(
+            field.get_internal_type(),
+            field.db_type(self.connection),
+        )
+
+    def _field_base_data_types(self, field):
+        # Yield base data types for array fields.
+        if field.base_field.get_internal_type() == 'ArrayField':
+            yield from self._field_base_data_types(field.base_field)
+        else:
+            yield self._field_data_type(field.base_field)
+
     def _create_like_index_sql(self, model, field):
         """
         Return the statement to create an index with varchar operator pattern
@@ -57,17 +75,28 @@ class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):
         return None
 
     def _alter_column_type_sql(self, model, old_field, new_field, new_type):
-        """Make ALTER TYPE with SERIAL make sense."""
-        table = model._meta.db_table
-        if new_type.lower() in ("serial", "bigserial"):
-            column = new_field.column
+        self.sql_alter_column_type = 'ALTER COLUMN %(column)s TYPE %(type)s'
+        # Cast when data type changed.
+        using_sql = ' USING %(column)s::%(type)s'
+        new_internal_type = new_field.get_internal_type()
+        old_internal_type = old_field.get_internal_type()
+        if new_internal_type == 'ArrayField' and new_internal_type == old_internal_type:
+            # Compare base data types for array fields.
+            if list(self._field_base_data_types(old_field)) != list(self._field_base_data_types(new_field)):
+                self.sql_alter_column_type += using_sql
+        elif self._field_data_type(old_field) != self._field_data_type(new_field):
+            self.sql_alter_column_type += using_sql
+        # Make ALTER TYPE with SERIAL make sense.
+        table = strip_quotes(model._meta.db_table)
+        serial_fields_map = {'bigserial': 'bigint', 'serial': 'integer'}
+        if new_type.lower() in serial_fields_map:
+            column = strip_quotes(new_field.column)
             sequence_name = "%s_%s_seq" % (table, column)
-            col_type = "integer" if new_type.lower() == "serial" else "bigint"
             return (
                 (
                     self.sql_alter_column_type % {
                         "column": self.quote_name(column),
-                        "type": col_type,
+                        "type": serial_fields_map[new_type.lower()],
                     },
                     [],
                 ),
@@ -102,8 +131,29 @@ class DatabaseSchemaEditor(BaseDatabaseSchemaEditor):
                         },
                         [],
                     ),
+                    (
+                        self.sql_set_sequence_owner % {
+                            'table': self.quote_name(table),
+                            'column': self.quote_name(column),
+                            'sequence': self.quote_name(sequence_name),
+                        },
+                        [],
+                    ),
                 ],
             )
+        elif old_field.db_parameters(connection=self.connection)['type'] in serial_fields_map:
+            # Drop the sequence if migrating away from AutoField.
+            column = strip_quotes(new_field.column)
+            sequence_name = '%s_%s_seq' % (table, column)
+            fragment, _ = super()._alter_column_type_sql(model, old_field, new_field, new_type)
+            return fragment, [
+                (
+                    self.sql_delete_sequence % {
+                        'sequence': self.quote_name(sequence_name),
+                    },
+                    [],
+                ),
+            ]
         else:
             return super()._alter_column_type_sql(model, old_field, new_field, new_type)
 
--- a/django/db/backends/postgresql/utils.py
+++ b/django/db/backends/postgresql/utils.py
@@ -1,7 +0,0 @@
-from django.utils.timezone import utc
-
-
-def utc_tzinfo_factory(offset):
-    if offset != 0:
-        raise AssertionError("database connection isn't set to UTC")
-    return utc
--- a/django/db/backends/utils.py
+++ b/django/db/backends/utils.py
@@ -3,11 +3,9 @@ import decimal
 import functools
 import hashlib
 import logging
-from time import time
+import time
 
-from django.conf import settings
 from django.db.utils import NotSupportedError
-from django.utils.timezone import utc
 
 logger = logging.getLogger('django.db.backends')
 
@@ -94,11 +92,11 @@ class CursorDebugWrapper(CursorWrapper):
     # XXX callproc isn't instrumented at this time.
 
     def execute(self, sql, params=None):
-        start = time()
+        start = time.monotonic()
         try:
             return super().execute(sql, params)
         finally:
-            stop = time()
+            stop = time.monotonic()
             duration = stop - start
             sql = self.db.ops.last_executed_query(self.cursor, sql, params)
             self.db.queries_log.append({
@@ -111,11 +109,11 @@ class CursorDebugWrapper(CursorWrapper):
             )
 
     def executemany(self, sql, param_list):
-        start = time()
+        start = time.monotonic()
         try:
             return super().executemany(sql, param_list)
         finally:
-            stop = time()
+            stop = time.monotonic()
             duration = stop - start
             try:
                 times = len(param_list)
@@ -170,11 +168,10 @@ def typecast_timestamp(s):  # does NOT store time zone information
         seconds, microseconds = seconds.split('.')
     else:
         microseconds = '0'
-    tzinfo = utc if settings.USE_TZ else None
     return datetime.datetime(
         int(dates[0]), int(dates[1]), int(dates[2]),
         int(times[0]), int(times[1]), int(seconds),
-        int((microseconds + '000000')[:6]), tzinfo
+        int((microseconds + '000000')[:6])
     )
 
 
--- a/django/db/migrations/executor.py
+++ b/django/db/migrations/executor.py
@@ -364,10 +364,12 @@ class MigrationExecutor:
                         found_add_field_migration = True
                         continue
 
-                column_names = [
-                    column.name for column in
-                    self.connection.introspection.get_table_description(self.connection.cursor(), table)
-                ]
+                column_names = []
+                with self.connection.cursor() as cursor:
+                    column_names = [
+                        column.name for column in
+                        self.connection.introspection.get_table_description(cursor, table)
+                    ]
                 if field.column not in column_names:
                     return False, project_state
                 found_add_field_migration = True
--- a/django/db/migrations/loader.py
+++ b/django/db/migrations/loader.py
@@ -271,7 +271,7 @@ class MigrationLoader:
                         ),
                         exc.node
                     ) from exc
-            raise exc
+            raise
         self.graph.ensure_not_cyclic()
 
     def check_consistent_history(self, connection):
--- a/django/db/migrations/recorder.py
+++ b/django/db/migrations/recorder.py
@@ -53,7 +53,9 @@ class MigrationRecorder:
 
     def has_table(self):
         """Return True if the django_migrations table exists."""
-        return self.Migration._meta.db_table in self.connection.introspection.table_names(self.connection.cursor())
+        with self.connection.cursor() as cursor:
+            tables = self.connection.introspection.table_names(cursor)
+        return self.Migration._meta.db_table in tables
 
     def ensure_schema(self):
         """Ensure the table exists and has the correct schema."""
--- a/django/db/models/functions/datetime.py
+++ b/django/db/models/functions/datetime.py
@@ -41,6 +41,8 @@ class Extract(TimezoneMixin, Transform):
         super().__init__(expression, **extra)
 
     def as_sql(self, compiler, connection):
+        if not connection.ops.extract_trunc_lookup_pattern.fullmatch(self.lookup_name):
+            raise ValueError("Invalid lookup_name: %s" % self.lookup_name)
         sql, params = compiler.compile(self.lhs)
         lhs_output_field = self.lhs.output_field
         if isinstance(lhs_output_field, DateTimeField):
@@ -175,14 +177,20 @@ class TruncBase(TimezoneMixin, Transform):
         super().__init__(expression, output_field=output_field, **extra)
 
     def as_sql(self, compiler, connection):
+        if not connection.ops.extract_trunc_lookup_pattern.fullmatch(self.kind):
+            raise ValueError("Invalid kind: %s" % self.kind)
         inner_sql, inner_params = compiler.compile(self.lhs)
-        if isinstance(self.output_field, DateTimeField):
+        tzname = None
+        if isinstance(self.lhs.output_field, DateTimeField):
             tzname = self.get_tzname()
+        elif self.tzinfo is not None:
+            raise ValueError('tzinfo can only be used with DateTimeField.')
+        if isinstance(self.output_field, DateTimeField):
             sql = connection.ops.datetime_trunc_sql(self.kind, inner_sql, tzname)
         elif isinstance(self.output_field, DateField):
-            sql = connection.ops.date_trunc_sql(self.kind, inner_sql)
+            sql = connection.ops.date_trunc_sql(self.kind, inner_sql, tzname)
         elif isinstance(self.output_field, TimeField):
-            sql = connection.ops.time_trunc_sql(self.kind, inner_sql)
+            sql = connection.ops.time_trunc_sql(self.kind, inner_sql, tzname)
         else:
             raise ValueError('Trunc only valid on DateField, TimeField, or DateTimeField.')
         return sql, inner_params
--- a/django/db/models/functions/text.py
+++ b/django/db/models/functions/text.py
@@ -4,22 +4,6 @@ from django.db.models.functions import Coalesce
 from django.db.models.lookups import Transform
 
 
-class BytesToCharFieldConversionMixin:
-    """
-    Convert CharField results from bytes to str.
-
-    MySQL returns long data types (bytes) instead of chars when it can't
-    determine the length of the result string. For example:
-        LPAD(column1, CHAR_LENGTH(column2), ' ')
-    returns the LONGTEXT (bytes) instead of VARCHAR.
-    """
-    def convert_value(self, value, expression, connection):
-        if connection.features.db_functions_convert_bytes_to_str:
-            if self.output_field.get_internal_type() == 'CharField' and isinstance(value, bytes):
-                return value.decode()
-        return super().convert_value(value, expression, connection)
-
-
 class Chr(Transform):
     function = 'CHR'
     lookup_name = 'chr'
@@ -136,7 +120,7 @@ class Lower(Transform):
     lookup_name = 'lower'
 
 
-class LPad(BytesToCharFieldConversionMixin, Func):
+class LPad(Func):
     function = 'LPAD'
 
     def __init__(self, expression, length, fill_text=Value(' '), **extra):
@@ -162,7 +146,7 @@ class Ord(Transform):
         return super().as_sql(compiler, connection, function='UNICODE', **extra_context)
 
 
-class Repeat(BytesToCharFieldConversionMixin, Func):
+class Repeat(Func):
     function = 'REPEAT'
 
     def __init__(self, expression, number, **extra):
--- a/django/db/models/query.py
+++ b/django/db/models/query.py
@@ -574,13 +574,13 @@ class QuerySet:
                 params = {k: v() if callable(v) else v for k, v in params.items()}
                 obj = self.create(**params)
             return obj, True
-        except IntegrityError as e:
+        except IntegrityError:
             try:
                 qs = self.select_for_update() if lock else self
                 return qs.get(**lookup), False
             except self.model.DoesNotExist:
                 pass
-            raise e
+            raise
 
     def _extract_model_params(self, defaults, **kwargs):
         """
@@ -960,7 +960,7 @@ class QuerySet:
             return self
         return self._combinator_query('difference', *other_qs)
 
-    def select_for_update(self, nowait=False, skip_locked=False, of=()):
+    def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
         """
         Return a new QuerySet instance that will select objects with a
         FOR UPDATE lock.
@@ -973,6 +973,7 @@ class QuerySet:
         obj.query.select_for_update_nowait = nowait
         obj.query.select_for_update_skip_locked = skip_locked
         obj.query.select_for_update_of = of
+        obj.query.select_for_no_key_update = no_key
         return obj
 
     def select_related(self, *fields):
--- a/django/db/models/sql/compiler.py
+++ b/django/db/models/sql/compiler.py
@@ -180,7 +180,11 @@ class SQLCompiler:
             # database views on which the optimization might not be allowed.
             pks = {
                 expr for expr in expressions
-                if hasattr(expr, 'target') and expr.target.primary_key and expr.target.model._meta.managed
+                if (
+                    hasattr(expr, 'target') and
+                    expr.target.primary_key and
+                    self.connection.features.allows_group_by_selected_pks_on_model(expr.target.model)
+                )
             }
             aliases = {expr.alias for expr in pks}
             expressions = [
@@ -525,19 +529,26 @@ class SQLCompiler:
                     nowait = self.query.select_for_update_nowait
                     skip_locked = self.query.select_for_update_skip_locked
                     of = self.query.select_for_update_of
-                    # If it's a NOWAIT/SKIP LOCKED/OF query but the backend
-                    # doesn't support it, raise NotSupportedError to prevent a
-                    # possible deadlock.
+                    no_key = self.query.select_for_no_key_update
+                    # If it's a NOWAIT/SKIP LOCKED/OF/NO KEY query but the
+                    # backend doesn't support it, raise NotSupportedError to
+                    # prevent a possible deadlock.
                     if nowait and not self.connection.features.has_select_for_update_nowait:
                         raise NotSupportedError('NOWAIT is not supported on this database backend.')
                     elif skip_locked and not self.connection.features.has_select_for_update_skip_locked:
                         raise NotSupportedError('SKIP LOCKED is not supported on this database backend.')
                     elif of and not self.connection.features.has_select_for_update_of:
                         raise NotSupportedError('FOR UPDATE OF is not supported on this database backend.')
+                    elif no_key and not self.connection.features.has_select_for_no_key_update:
+                        raise NotSupportedError(
+                            'FOR NO KEY UPDATE is not supported on this '
+                            'database backend.'
+                        )
                     for_update_part = self.connection.ops.for_update_sql(
                         nowait=nowait,
                         skip_locked=skip_locked,
                         of=self.get_select_for_update_of_arguments(),
+                        no_key=no_key,
                     )
 
                 if for_update_part and self.connection.features.for_update_after_from:
--- a/django/db/models/sql/query.py
+++ b/django/db/models/sql/query.py
@@ -198,6 +198,7 @@ class Query:
         self.select_for_update_nowait = False
         self.select_for_update_skip_locked = False
         self.select_for_update_of = ()
+        self.select_for_no_key_update = False
 
         self.select_related = False
         # Arbitrary limit for select_related to prevents infinite recursion.
--- a/django/db/models/sql/subqueries.py
+++ b/django/db/models/sql/subqueries.py
@@ -22,7 +22,10 @@ class DeleteQuery(Query):
         self.alias_map = {table: self.alias_map[table]}
         self.where = where
         cursor = self.get_compiler(using).execute_sql(CURSOR)
-        return cursor.rowcount if cursor else 0
+        if cursor:
+            with cursor:
+                return cursor.rowcount
+        return 0
 
     def delete_batch(self, pk_list, using):
         """
@@ -73,7 +76,10 @@ class DeleteQuery(Query):
             self.where = self.where_class()
             self.add_q(Q(pk__in=values))
         cursor = self.get_compiler(using).execute_sql(CURSOR)
-        return cursor.rowcount if cursor else 0
+        if cursor:
+            with cursor:
+                return cursor.rowcount
+        return 0
 
 
 class UpdateQuery(Query):
--- a/django/forms/fields.py
+++ b/django/forms/fields.py
@@ -523,6 +523,9 @@ class EmailField(CharField):
     default_validators = [validators.validate_email]
 
     def __init__(self, **kwargs):
+        # The default maximum length of an email is 320 characters per RFC 3696
+        # section 3.
+        kwargs.setdefault("max_length", 320)
         super().__init__(strip=True, **kwargs)
 
 
--- a/django/forms/widgets.py
+++ b/django/forms/widgets.py
@@ -373,16 +373,40 @@ class MultipleHiddenInput(HiddenInput):
 
 class FileInput(Input):
     input_type = 'file'
+    allow_multiple_selected = False
     needs_multipart_form = True
     template_name = 'django/forms/widgets/file.html'
 
+    def __init__(self, attrs=None):
+        if (
+            attrs is not None and
+            not self.allow_multiple_selected and
+            attrs.get("multiple", False)
+        ):
+            raise ValueError(
+                "%s doesn't support uploading multiple files."
+                % self.__class__.__qualname__
+            )
+        if self.allow_multiple_selected:
+            if attrs is None:
+                attrs = {"multiple": True}
+            else:
+                attrs.setdefault("multiple", True)
+        super().__init__(attrs)
+
     def format_value(self, value):
         """File input never renders a value."""
         return
 
     def value_from_datadict(self, data, files, name):
         "File widgets take data from FILES, not POST"
-        return files.get(name)
+        getter = files.get
+        if self.allow_multiple_selected:
+            try:
+                getter = files.getlist
+            except AttributeError:
+                pass
+        return getter(name)
 
     def value_omitted_from_data(self, data, files, name):
         return name not in files
--- a/django/http/multipartparser.py
+++ b/django/http/multipartparser.py
@@ -13,6 +13,7 @@ from urllib.parse import unquote
 from django.conf import settings
 from django.core.exceptions import (
     RequestDataTooBig, SuspiciousMultipartForm, TooManyFieldsSent,
+    TooManyFilesSent,
 )
 from django.core.files.uploadhandler import (
     SkipFile, StopFutureHandlers, StopUpload,
@@ -37,6 +38,7 @@ class InputStreamExhausted(Exception):
 RAW = "raw"
 FILE = "file"
 FIELD = "field"
+FIELD_TYPES = frozenset([FIELD, RAW])
 
 
 class MultiPartParser:
@@ -98,6 +100,22 @@ class MultiPartParser:
         self._upload_handlers = upload_handlers
 
     def parse(self):
+        # Call the actual parse routine and close all open files in case of
+        # errors. This is needed because if exceptions are thrown the
+        # MultiPartParser will not be garbage collected immediately and
+        # resources would be kept alive. This is only needed for errors because
+        # the Request object closes all uploaded files at the end of the
+        # request.
+        try:
+            return self._parse()
+        except Exception:
+            if hasattr(self, "_files"):
+                for _, files in self._files.lists():
+                    for fileobj in files:
+                        fileobj.close()
+            raise
+
+    def _parse(self):
         """
         Parse the POST data and break it into a FILES MultiValueDict and a POST
         MultiValueDict.
@@ -143,6 +161,8 @@ class MultiPartParser:
         num_bytes_read = 0
         # To count the number of keys in the request.
         num_post_keys = 0
+        # To count the number of files in the request.
+        num_files = 0
         # To limit the amount of data read from the request.
         read_size = None
 
@@ -155,6 +175,20 @@ class MultiPartParser:
                     self.handle_file_complete(old_field_name, counters)
                     old_field_name = None
 
+                if (
+                    item_type in FIELD_TYPES and
+                    settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None
+                ):
+                    # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.
+                    num_post_keys += 1
+                    # 2 accounts for empty raw fields before and after the
+                    # last boundary.
+                    if settings.DATA_UPLOAD_MAX_NUMBER_FIELDS + 2 < num_post_keys:
+                        raise TooManyFieldsSent(
+                            "The number of GET/POST parameters exceeded "
+                            "settings.DATA_UPLOAD_MAX_NUMBER_FIELDS."
+                        )
+
                 try:
                     disposition = meta_data['content-disposition'][1]
                     field_name = disposition['name'].strip()
@@ -167,15 +201,6 @@ class MultiPartParser:
                 field_name = force_text(field_name, encoding, errors='replace')
 
                 if item_type == FIELD:
-                    # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FIELDS.
-                    num_post_keys += 1
-                    if (settings.DATA_UPLOAD_MAX_NUMBER_FIELDS is not None and
-                            settings.DATA_UPLOAD_MAX_NUMBER_FIELDS < num_post_keys):
-                        raise TooManyFieldsSent(
-                            'The number of GET/POST parameters exceeded '
-                            'settings.DATA_UPLOAD_MAX_NUMBER_FIELDS.'
-                        )
-
                     # Avoid reading more than DATA_UPLOAD_MAX_MEMORY_SIZE.
                     if settings.DATA_UPLOAD_MAX_MEMORY_SIZE is not None:
                         read_size = settings.DATA_UPLOAD_MAX_MEMORY_SIZE - num_bytes_read
@@ -201,6 +226,16 @@ class MultiPartParser:
 
                     self._post.appendlist(field_name, force_text(data, encoding, errors='replace'))
                 elif item_type == FILE:
+                    # Avoid storing more than DATA_UPLOAD_MAX_NUMBER_FILES.
+                    num_files += 1
+                    if (
+                        settings.DATA_UPLOAD_MAX_NUMBER_FILES is not None and
+                        num_files > settings.DATA_UPLOAD_MAX_NUMBER_FILES
+                    ):
+                        raise TooManyFilesSent(
+                            "The number of files exceeded "
+                            "settings.DATA_UPLOAD_MAX_NUMBER_FILES."
+                        )
                     # This is a file, use the handler...
                     file_name = disposition.get('filename')
                     if file_name:
@@ -268,8 +303,13 @@ class MultiPartParser:
                         # Handle file upload completions on next iteration.
                         old_field_name = field_name
                 else:
-                    # If this is neither a FIELD or a FILE, just exhaust the stream.
-                    exhaust(stream)
+                    # If this is neither a FIELD nor a FILE, exhaust the field
+                    # stream. Note: There could be an error here at some point,
+                    # but there will be at least two RAW types (before and
+                    # after the other boundaries). This branch is usually not
+                    # reached at all, because a missing content-disposition
+                    # header will skip the whole boundary.
+                    exhaust(field_stream)
         except StopUpload as e:
             self._close_files()
             if not e.connection_reset:
--- a/django/http/request.py
+++ b/django/http/request.py
@@ -11,7 +11,7 @@ from django.core.exceptions import (
     DisallowedHost, ImproperlyConfigured, RequestDataTooBig,
 )
 from django.core.files import uploadhandler
-from django.http.multipartparser import MultiPartParser, MultiPartParserError
+from django.http.multipartparser import MultiPartParser, MultiPartParserError, TooManyFilesSent
 from django.utils.datastructures import (
     CaseInsensitiveMapping, ImmutableList, MultiValueDict,
 )
@@ -313,7 +313,7 @@ class HttpRequest:
                 data = self
             try:
                 self._post, self._files = self.parse_file_upload(self.META, data)
-            except MultiPartParserError:
+            except (MultiPartParserError, TooManyFilesSent):
                 # An error occurred while parsing POST data. Since when
                 # formatting the error the request handler might access
                 # self.POST, set self._post and self._file to prevent
--- a/django/http/response.py
+++ b/django/http/response.py
@@ -442,7 +442,9 @@ class FileResponse(StreamingHttpResponse):
             if filename:
                 try:
                     filename.encode('ascii')
-                    file_expr = 'filename="{}"'.format(filename)
+                    file_expr = 'filename="{}"'.format(
+                        filename.replace('\\', '\\\\').replace('"', r'\"')
+                    )
                 except UnicodeEncodeError:
                     file_expr = "filename*=utf-8''{}".format(quote(filename))
                 self['Content-Disposition'] = 'attachment; {}'.format(file_expr)
--- a/django/urls/resolvers.py
+++ b/django/urls/resolvers.py
@@ -289,7 +289,7 @@ class LocalePrefixPattern:
     @property
     def regex(self):
         # This is only used by reverse() and cached in _reverse_dict.
-        return re.compile(self.language_prefix)
+        return re.compile(re.escape(self.language_prefix))
 
     @property
     def language_prefix(self):
--- a/django/utils/functional.py
+++ b/django/utils/functional.py
@@ -3,8 +3,6 @@ import itertools
 import operator
 from functools import total_ordering, wraps
 
-from django.utils.version import PY36, get_docs_version
-
 
 # You can't trivially replace this with `functools.partial` because this binds
 # to classes and returns bound instances, whereas functools.partial (on
@@ -34,29 +32,8 @@ class cached_property:
             '__set_name__() on it.'
         )
 
-    @staticmethod
-    def _is_mangled(name):
-        return name.startswith('__') and not name.endswith('__')
-
     def __init__(self, func, name=None):
-        if PY36:
-            self.real_func = func
-        else:
-            func_name = func.__name__
-            name = name or func_name
-            if not (isinstance(name, str) and name.isidentifier()):
-                raise ValueError(
-                    "%r can't be used as the name of a cached_property." % name,
-                )
-            if self._is_mangled(name):
-                raise ValueError(
-                    'cached_property does not work with mangled methods on '
-                    'Python < 3.6 without the appropriate `name` argument. See '
-                    'https://docs.djangoproject.com/en/%s/ref/utils/'
-                    '#cached-property-mangled-name' % get_docs_version(),
-                )
-            self.name = name
-            self.func = func
+        self.real_func = func
         self.__doc__ = getattr(func, '__doc__')
 
     def __set_name__(self, owner, name):
--- a/django/utils/module_loading.py
+++ b/django/utils/module_loading.py
@@ -72,11 +72,10 @@ def module_has_submodule(package, module_name):
     full_module_name = package_name + '.' + module_name
     try:
         return importlib_find(full_module_name, package_path) is not None
-    except (ImportError, AttributeError):
-        # When module_name is an invalid dotted path, Python raises ImportError
-        # (or ModuleNotFoundError in Python 3.6+). AttributeError may be raised
+    except (ModuleNotFoundError, AttributeError):
+        # When module_name is an invalid dotted path, Python raises
+        # ModuleNotFoundError. AttributeError is raised on PY36 (fixed in PY37)
         # if the penultimate part of the path is not a package.
-        # (https://bugs.python.org/issue30436)
         return False
 
 
--- a/django/utils/regex_helper.py
+++ b/django/utils/regex_helper.py
@@ -5,6 +5,10 @@ Used internally by Django and not intended for external use.
 This is not, and is not intended to be, a complete reg-exp decompiler. It
 should be good enough for a large class of URLS, however.
 """
+import re
+
+from django.utils.functional import SimpleLazyObject
+
 # Mapping of an escape character to a representative of that class. So, e.g.,
 # "\w" is replaced by "x" in a reverse URL. A value of None means to ignore
 # this sequence. Any missing key is mapped to itself.
@@ -331,3 +335,17 @@ def flatten_result(source):
         for i in range(len(result)):
             result[i] += piece
     return result, result_args
+
+
+def _lazy_re_compile(regex, flags=0):
+    """Lazily compile a regex with flags."""
+    def _compile():
+        # Compile the regex if it was not passed pre-compiled.
+        if isinstance(regex, (str, bytes)):
+            return re.compile(regex, flags)
+        else:
+            assert not flags, (
+                'flags must be empty if regex is passed pre-compiled'
+            )
+            return regex
+    return SimpleLazyObject(_compile)
--- a/django/utils/translation/trans_real.py
+++ b/django/utils/translation/trans_real.py
@@ -29,6 +29,11 @@ _default = None
 # magic gettext number to separate context from message
 CONTEXT_SEPARATOR = "\x04"
 
+# Maximum number of characters that will be parsed from the Accept-Language
+# header to prevent possible denial of service or memory exhaustion attacks.
+# About 10x longer than the longest value shown on MDNs Accept-Language page.
+ACCEPT_LANGUAGE_HEADER_MAX_LENGTH = 500
+
 # Format of Accept-Language header values. From RFC 2616, section 14.4 and 3.9
 # and RFC 3066, section 2.1
 accept_language_re = re.compile(r'''
@@ -560,7 +565,7 @@ def get_language_from_request(request, check_path=False):
 
 
 @functools.lru_cache(maxsize=1000)
-def parse_accept_lang_header(lang_string):
+def _parse_accept_lang_header(lang_string):
     """
     Parse the lang_string, which is the body of an HTTP Accept-Language
     header, and return a tuple of (lang, q-value), ordered by 'q' values.
@@ -582,3 +587,27 @@ def parse_accept_lang_header(lang_string):
         result.append((lang, priority))
     result.sort(key=lambda k: k[1], reverse=True)
     return tuple(result)
+
+def parse_accept_lang_header(lang_string):
+    """
+    Parse the value of the Accept-Language header up to a maximum length.
+
+    The value of the header is truncated to a maximum length to avoid potential
+    denial of service and memory exhaustion attacks. Excessive memory could be
+    used if the raw value is very large as it would be cached due to the use of
+    functools.lru_cache() to avoid repetitive parsing of common header values.
+    """
+    # If the header value doesn't exceed the maximum allowed length, parse it.
+    if len(lang_string) <= ACCEPT_LANGUAGE_HEADER_MAX_LENGTH:
+        return _parse_accept_lang_header(lang_string)
+
+    # If there is at least one comma in the value, parse up to the last comma
+    # before the max length, skipping any truncated parts at the end of the
+    # header value.
+    index = lang_string.rfind(",", 0, ACCEPT_LANGUAGE_HEADER_MAX_LENGTH)
+    if index > 0:
+        return _parse_accept_lang_header(lang_string[:index])
+
+    # Don't attempt to parse if there is only one language-range value which is
+    # longer than the maximum allowed length and so truncated.
+    return ()
